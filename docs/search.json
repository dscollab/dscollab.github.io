[
  {
    "objectID": "Pages/blog.html",
    "href": "Pages/blog.html",
    "title": "\nWelcome to our Blog!\n",
    "section": "",
    "text": "Welcome to our Blog!\n\n\nTake a look at some of our past socials and events!\n\n\n\n2025\n\n\n\nProject Jam\n\n\nWednesday, April 16th, 2025 @ SRB\n\n \n\nThanks to everyone who showed up to form groups and start on projects. We can’t wait to see the results of each groups’ efforts at the project showcase! Contact any officer if you have questions about group formation, or your projects, and stay tuned over the coming weeks for future project jams!\n\n\n\n\nAppFolio Tour\n\n\nFriday, February 21, 2025 @ AppFolio\n\n  \n\nOn Friday, February 21st, about 30 staff and club members attended the AppFolio Goleta Office for a tour of the premises ending in a presentation and Q&A! We learned about the structure of the company, as well as the many ways that data science plays a key role in AppFolio’s success. AppFolio is a company that offers SaaS (Software as a Service), across 12 distinct products. These products are made possible by the diverse teams that maintain them which are comprised of data analysts, operations-oriented data scientists, data engineers, and software engineers to name a few. This was an eye-opening experience that demonstrated the impact of a data-driven business model. Stay tuned for future workshops & events!\n\n\n\n\nStats R Fun #5\n\n\nThursday, February 6, 2025 @ Psych #1924\n\n  \n\nOn February 6, 2025, DSCollab hosted the fifth workshop of the year! Our graduate advisor, Ethan Marzban, guided us through an intuitive yet thorough introduction to machine learning. We explored the topics of model fitting, testing, and validation, supervised and unsupervised learning, and discussion of implementation for projects. We hope to see you all at future workshops and events!\n\n\n\n\nStats R Fun #4\n\n\nThursday, January 21, 2025 @ TD-W 2600\n\n\n\nOn January 21, 2025, DSCollab hosted the fourth workshop of the year and the first of 2025! We explored data cleaning in python through google colab, and explored the process of converting difficult-to-manage data into a project worthy dataset! Check out the meetings page for links to the slides and material. See you all at the next one!\n\n\n\n2024\n\n\n\n\nStats R Fun #3\n\n\nWednesday, November 13, 2024 @ Psych #1924\n\n \n\nOn November 30, 2024, DSCollab hosted its third workshop of the academic year. Our graduate advisor, Ethan Marzban, presented an introduction to data visualization as a broad topic, and led a walkthrough of the popular Python library Altair! Thanks so much to everyone who showed up; the interactive worksheet and answers can be found on the meetings page!\n\n\n\n\n\nStats R Fun #2\n\n\nThursday, October 30, 2024 @ Psych #1924\n\n  \n\nOn Wednesday 30, 2024, DSCollab hosted its second workshop of the academic year. We delved into two of the most important and foundational structures for working with data in Python: NumPy & Pandas. These two Python packages allow for intuitive and powerful data wrangling and manipulation, and they are accessible to anyone, regardless of prior data science experience. Thank you to everyone who showed up and stay tuned for future workshops where we will explore the vast world of data science through data visualization and other exciting topics!\n\n\n\n\n\nBoba + Study Social\n\n\nFriday, October 25, 2024 @ Ai Cha & SRB\n\n  \n\nOn Friday afternoon, October 25th, DSCollab had the pleasure of hosting a study jam & boba social sponsored by Ai Cha! From 2:30-3:00 pm, participants were treated to discounted boba at Ai Cha, and alongside the DSCollab staff, had in a social get-together. Then, the group walked over to the Student Resource Building and had a study jam in the 2nd Floor Conference Room from 3:00-5:00 pm. They were treated to free snacks, the expertise and advice of the staff for various stats classes, and lofi hip-hop beats to study to! Thanks to everyone who came, and for those that missed out, we hope to see you at future events!\n\n\n\n\n\nStats R Fun #1\n\n\nThursday, October 17, 2024 @ Psych #1902\n\n  \n\nOn Thursday, October 17th, at 6:30 pm, DSCollab hosted its first workshop of the 2024-2025 academic year. Our president, Cyrus Navasca, introduced us to many of the essential characteristics and components of python for data science! We delved into basic python syntax, data types, and importing data.\n\n\n\n\n\nDiscover Guest Speaker\n\n\nWednesday, October 16, 2024 on Zoom\n\n \n\nOn Wednesday, October 16th, from 7-8 pm, DSCollab and Delta Sigma Pi hosted a guest speaker event by Brianna Griffin, a Senior Lead Data Analyst @ Discover! Brianna shared her experiences studying at UC Santa Barbara, going through the internship process, and landing a job in the fraud prevention sector at Discover Financial Services. She discussed her day-to-day life, her role and responsibilities, and her career progression through the company. Overall, it was a very worthwhile and informative event, especially since Brianna was in our shoes not too long ago!\n\n\n\n\n\nGeneral Meeting #1\n\n\nWednesday, October 9, 2024 @ Psych #1924\n\n  \n\nOn Wednesday, October 9th, the officers of DS Collab hosted the first general meeting of the 2024-2025 school year! We had a great turnout of students from a variety of majors and backgrounds who were all excited to learn data science from like-minded individuals. The meeting began with introductions from our officers and turned into details about future meetings, workshops, and events. Then, we ended the night with a DS Collab / PSTAT Department / UCSB Trivia related Kahoot with giftcard prizes from the sponsor of the meeting: Woodstock’s Pizza!\n\n\n\n\n\nIce Cream Social\n\n\nFriday, October 4, 2024 @ SRB Lawn\n\n\n\nOn Friday, October 4th, DS Collab hosted a club kickoff ice cream social at the SRB Lawn! From 3:00 to 5:00 pm, the officers connected with many current and prospective club members about the exciting world of data science! We spoke about the structure of the club, the general plan for the year, and the plethora of ways that beginners and non-majors can take their first steps. During the social, we offered a variety of ice cream treats, including classic Kirkland chocolate options and vegan alternatives. The event had a successful turnout, with students mingling, making new acquaintances, and nurturing a strong sense of the data science community through this enjoyable gathering.\n\n\n\n\n\nProfessor Speaker Panel\n\n\nMonday, April 8, 2024 @ Arts Building 1349\n\n\nOn Monday, April 8th, at the UCSB Arts Building, we had the honor of hosting two esteemed professors in Statistics and Data Science, Dr. Katie Coburn and Dr. Rituparna Sen, for our Professor Panel event in collaboration with UCSB’s Delta Sigma Pi. Throughout the panel, both professors shared their experiences and perspectives on Statistics and Data Science and offered insights into the current industry landscape, as well as advice on pursuing an academic career or extended education in the field. A highlight of the event was the emphasis placed on pursuing an academic route within Statistics and Data Science. Dr. Coburn and Dr. Sen provided invaluable insights into the academic journey, discussing topics such as graduate studies, research opportunities, and the importance of mentorship in shaping one’s scholarly pursuits. Approximately 40 students were able to get questions answered and gain a deeper understanding and inspiration for their academic and professional endeavors in Statistics and Data Science.\n\n\n\n\n\nMOSAIKS Speaker Event\n\n\nJanuary 19, 2024\n\n \n\nOn Friday, January 19th, at UCSB’s North Hall, we had the pleasure of hosting speaker Carlo Broderick, who received a Master’s degree in environmental data science at UC Santa Barbara’s Bren School of Environmental Science and Management. He came to speak about MOSAIKS (Multi-task Observation using Satellite Imagery & Kitchen Sinks), a new approach to analyzing satellite imagery with machine learning, for our Data Science Collaborative members.\nThe event provided a valuable opportunity for our curious-minded members to delve into the intricacies of MOSAIKS and its transformative potential. Carlo elucidated how MOSAIKS transforms satellite images into tabular data (features). One of the most remarkable aspects highlighted was MOSAICS’ remarkable ability to generalize across diverse tasks using a unified set of features, ensuring accessibility and effectiveness across a broad spectrum of applications.\nThrough engaging with Carlo during the speaker event, our members gained a deeper understanding of MOSAICS’ underlying principles and its practical implications. Carlo’s expertise and passion for leveraging machine learning to address real-world challenges shone through, inspiring our members to explore the intersection of data science and environmental analysis further."
  },
  {
    "objectID": "Pages/about.html",
    "href": "Pages/about.html",
    "title": "About",
    "section": "",
    "text": "Founded in 2023 by Kris Hao and Anya Macomber, the workshop series aims to provide an inclusive and collegial atmosphere where motivated students can engage more fully with real-world datasets and projects. Our goals as an organization are:\n\nTo make statistics accessible to all skill levels\nTo build a community around data science at UCSB\nEnsure the success of our members by guiding them through their own resume-building data science projects",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "Pages/meetings.html",
    "href": "Pages/meetings.html",
    "title": "2023-2024 Meetings",
    "section": "",
    "text": "Part 1: Fundamentals (Opens in a New Tab)\nPart 2: Data Types and Data Structures (Opens in a New Tab)\nPart 3: Functions (Opens in a New Tab)",
    "crumbs": [
      "Meetings",
      "2023-2024 Meetings"
    ]
  },
  {
    "objectID": "Pages/meetings.html#r-labs",
    "href": "Pages/meetings.html#r-labs",
    "title": "2023-2024 Meetings",
    "section": "",
    "text": "Part 1: Fundamentals (Opens in a New Tab)\nPart 2: Data Types and Data Structures (Opens in a New Tab)\nPart 3: Functions (Opens in a New Tab)",
    "crumbs": [
      "Meetings",
      "2023-2024 Meetings"
    ]
  },
  {
    "objectID": "Pages/meetings.html#general-meetings",
    "href": "Pages/meetings.html#general-meetings",
    "title": "2023-2024 Meetings",
    "section": "General Meetings",
    "text": "General Meetings\n\n\n\nGM 8 - API Workshop (4/24/24)\n\n\n\nSlides: TBD… [.pdf] (Click to download)\nAPI Demo1: [.py] (Click to download)\nAPI Demo2: [.py] (Click to download)\nAPI Demo3: [.py] (Click to download)\n\n\n\n\n\nGM 7 - Tableau Intro Workshop #2 (2/23/24)\n\n\n\nImage: [.pdf] (Click to download)\n\nResult: [.twb] (Click to download)\n\n\n\n\n\n\nGM 6 - Project Walkthrough (2/16/24)\n\n\n\nSlides: [google slides] (Click to view)\n\nNetlify Example: [link] (Click to view)\n\n\n\n\n\n\nGM 5 - Tableau Intro Workshop #1 (1/26/24)\n\n\n\nSlides: [.pdf] (Click to download)\n\n\n\n\n\n\nGM 4 - Data Cleaning + Github (11/02/23)\n\n\n\nSlides: [.pdf] (Click to download)\nData Cleaning Code Demo [.R] (Click to download)\n\n\n\n\n\n\n\nGM 3 - Data Visualization (10/19/23)\n\n\n\nDownload R and R Studio  \n\nSteps to Access the Interactive Demo/Tutorial:\n\nDownload the file located at https://bit.ly/dsc-gm3\nOpen the file in RStudio and follow any prompts to download any necessary packages\nClick the Run Document button: \n\n\n\n\n\n\n\n\nGM 2 - Descriptive Statistics & Project Proposal (10/12/23)\n\n\n\nSlides: [.pdf] (Click to download)  Descriptive Statistics (Click to View) \n\n\n\n\n\n\nGM 1 - Fall Kickoff! (10/05/23)\n\n\n\nSlides: [.pdf] (Click to download)",
    "crumbs": [
      "Meetings",
      "2023-2024 Meetings"
    ]
  },
  {
    "objectID": "Pages/meetings.html#other-resources",
    "href": "Pages/meetings.html#other-resources",
    "title": "2023-2024 Meetings",
    "section": "Other resources",
    "text": "Other resources\n\nPSTAT Department Resume Template  (Click to Download)\nProject Proposal Outline  (Click to Download)\nProject Proposal Example  (Click to Download)\nMOSAIKS Presentation  (Click to Download)\n\n\n\n\n\n\n\n\n1 / 8\n\n\n\nTableau Workshop\n\n\n\n\n2 / 8\n\n\n\nTableau Workshop\n\n\n\n\n3 / 8\n\n\n\nTableau Workshop\n\n\n\n\n4 / 8\n\n\n\nMOSAIKS Presentation\n\n\n\n\n5 / 8\n\n\n\nMOSAIKS Presentation\n\n\n\n\n6 / 8\n\n\n\nMOSAIKS Presentation\n\n\n\n\n7 / 8\n\n\n\nTableau Workshop 2\n\n\n\n\n8 / 8\n\n\n\nTableau Workshop",
    "crumbs": [
      "Meetings",
      "2023-2024 Meetings"
    ]
  },
  {
    "objectID": "Pages/projects2324.html",
    "href": "Pages/projects2324.html",
    "title": "2023-2024 Projects",
    "section": "",
    "text": "Bitcoin Price Prediction\n\n\n\n\n\n\nBitcoin Price Prediction\n\nHow does the price of Bitcoin change over time? In this project, a Long Short-Term Memory (LSTM) model was used to capture data and model long-term dependencies in sequential data.\nGroup Members: Alice Xu\n\n\n\n\n\n\n\n\n\n\n\n\n\nNBA Prediction Model\n\n\n\n\n\n\nNBA Prediction Model\n\nWhether attempting to win a bet with your friends, or trying to make a viral post before your favorite NBA team plays, we can clearly see the benefit of a predictive model for NBA match-ups!\nGroup Members: Akhil Gorla, Aditya Iyer\n\n \n\n\n\n\n\n\n\n\n\n\n\nPokemon Type Interactions with Tableau\n\n\n\n\n\n\nPokemon Type Interactions with Tableau\n\nThere are a lot of pokemon types and a plethora of various type combinations. Using tableau, this project allows for clear and concise examination of the various types and their interactions on different Pokemon!\nGroup Members: Pramukh Shankar\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictive Modeling of Heart Disease\n\n\n\n\n\n\nPredictive Modeling of Heart Disease\n\nUsing hospital data, this project sought to determine the links between various indicators and their impacts on the chances of a patient developing heart disease.\nGroup Members: Cyrus Navasca, Owen Feng, Nikhil Gupta, Tony Cao\n\n  \n\n\n\n\n\n\n\n\n\n\n\nVisual Mouse Using MediaPipe\n\n\n\n\n\n\nVisual Mouse Using MediaPipe\n\nUsing MediaPipe, a Raspberry Pi, and a webcam, the image data collected by the camera is translated into mouse movements and clicks!\nGroup Members: Colton Rowe\n\n  \n\n\n\n\n\n\n\n X",
    "crumbs": [
      "Projects",
      "2023-2024 Projects"
    ]
  },
  {
    "objectID": "Pages/meetings2425.html",
    "href": "Pages/meetings2425.html",
    "title": "2024-2025 Meetings",
    "section": "",
    "text": "GM 1 - Fall Kickoff! (10/09/24)\n\n\n\nSlides: [gm1.pdf] (Click to download)",
    "crumbs": [
      "Meetings",
      "2024-2025 Meetings"
    ]
  },
  {
    "objectID": "Pages/meetings2425.html#general-meetings",
    "href": "Pages/meetings2425.html#general-meetings",
    "title": "2024-2025 Meetings",
    "section": "",
    "text": "GM 1 - Fall Kickoff! (10/09/24)\n\n\n\nSlides: [gm1.pdf] (Click to download)",
    "crumbs": [
      "Meetings",
      "2024-2025 Meetings"
    ]
  },
  {
    "objectID": "Pages/meetings2425.html#stats-r-fun",
    "href": "Pages/meetings2425.html#stats-r-fun",
    "title": "2024-2025 Meetings",
    "section": "Stats R Fun",
    "text": "Stats R Fun\n\n\n\n\nStats R Fun #1 (10/17/24)\n\n\n\nSlides: [Click Here] (Opens in a New Tab) \n\nWorksheet: [IntroToPython.ipynb] (Click to download) \nAnswer Key: [IntroAnswers.ipynb] (Click to download) \n\n\n\n\n\n\n\nStats R Fun #2 (10/30/24)\n\n\n\nSlides: [Click Here] (Opens in a New Tab) \n\nWorksheet: [NumpyPandas.ipynb] (Click to download) \nAnswer Key: [NumpyPandasAnswers.ipynb] (Click to download) \n\n\n\n\n\n\n\nStats R Fun #3 (11/14/24)\n\n\n\nSlides: [Click Here] (Opens in a New Tab) \n\nWorksheet w/ Solutions: [DataVisualizationWithSolutions.ipynb] (Click to download) \n\n\n\n\n\n\n\nStats R Fun #4 (1/21/25)\n\n\n\nSlides: [Click Here] (Opens in a New Tab) \n\nWorksheet: [DataCleaningWorkshop.ipynb] (Click to download) \nAnswer Key: [DataCleaningANSWERS.ipynb] (Click to download) \n\n\n\n\n\n\n\nStats R Fun #5 (2/6/25)\n\n\n\nSlides: [Click Here] (Opens in a New Tab)",
    "crumbs": [
      "Meetings",
      "2024-2025 Meetings"
    ]
  },
  {
    "objectID": "Pages/projects2425.html",
    "href": "Pages/projects2425.html",
    "title": "2024-2025 Projects",
    "section": "",
    "text": "Spotify Audio Analysis & Predictive Modeling\n\n\n\n\n\n\nSpotify Audio Analysis & Predictive Modeling\n\nproject 1 description\nGroup Member(s): Wonyoung Hur, Jack Zhang, Joaquin Wong, Jerry Zheng, Chenghuan Ge, Darren Dai\n\n   \n\n\n\n\n\n\n\n\n\n\n\nNBA Subreddit Sentiment Analysis\n\n\n\n\n\n\nNBA Subreddit Sentiment Analysis\n\nproject 2 description\nGroup Member(s): Alexander Bloyer\n\n   \n\n\n\n\n\n\n\n\n\n\n\nSex & ADHD Prediction\n\n\n\n\n\n\nSex & ADHD Prediction\n\nproject 3 description\nGroup Members: Alice Xu & Ashlyn Lin\n\n   \n\n\n\n\n\n\n\n\n\n\n\nDiabetic Retinopathy Classification\n\n\n\n\n\n\nDiabetic Retinopathy Classification\n\nproject 4 description\nGroup Members: Cyrus Navasca, Owen Feng, Shashin Gupta, Nikhil Gupta\n\n    \n\n\n\n\n\n\n\n\n\n\n\nRaccoon Analysis of UCSB\n\n\n\n\n\n\nRaccoon Analysis of UCSB\n\nproject 5 description\nGroup Members: Katherine Hung, Sabrina Nguyen, Brendan Chung, Dylan Acosta, Joseph Cancio\n\n  \n\n\n\n\n\n\n\n X",
    "crumbs": [
      "Projects",
      "2024-2025 Projects"
    ]
  },
  {
    "objectID": "Files/R_Labs/L03_Fnts/R_l03.html",
    "href": "Files/R_Labs/L03_Fnts/R_l03.html",
    "title": "Programming in R",
    "section": "",
    "text": "Please make sure you have downloaded both R and RStudio, as outlined in the previous lab.\nPlease make sure you are familiar with the concepts of data types, data structures, and packages, along with the basics of programming outlined in Part 1."
  },
  {
    "objectID": "Files/R_Labs/L03_Fnts/R_l03.html#prerequisites",
    "href": "Files/R_Labs/L03_Fnts/R_l03.html#prerequisites",
    "title": "Programming in R",
    "section": "",
    "text": "Please make sure you have downloaded both R and RStudio, as outlined in the previous lab.\nPlease make sure you are familiar with the concepts of data types, data structures, and packages, along with the basics of programming outlined in Part 1."
  },
  {
    "objectID": "Files/R_Labs/L03_Fnts/R_l03.html#pre-existing-r-functions",
    "href": "Files/R_Labs/L03_Fnts/R_l03.html#pre-existing-r-functions",
    "title": "Programming in R",
    "section": "Pre-Existing R Functions",
    "text": "Pre-Existing R Functions\nRecall, from Part 1, that functions in R behave much like functions in mathematics. .\nGiven a function f() that has been defined in R, we call the function on inputs (aka arguments) arg1, arg2, … using the syntax\n\nf(arg1, arg2, ...)\n\nKeep in mind that each of the arguments could potentially be of different data types and/or structures, depending on how the function has been defined.\nWe have actually already seen and utilized several functions in R! For example, we used the print() function to print outputs; we also used the data.frame() function to create a data frame.\n\n\n\n\n\n\nTip\n\n\n\nRemember that you can always access the help file for a function with name function_name using the syntax ?function_name."
  },
  {
    "objectID": "Files/R_Labs/L03_Fnts/R_l03.html#user-defined-functions",
    "href": "Files/R_Labs/L03_Fnts/R_l03.html#user-defined-functions",
    "title": "Programming in R",
    "section": "User-Defined Functions",
    "text": "User-Defined Functions\nTo define a function with the name function_name, we use the syntax:\n\nfunction_name &lt;- function(arg1, arg2, ...) {\n  &lt;body of function&gt;\n}\n\nFor example,\n\nsum_sq &lt;- function(x) {\n  return(sum(x^2))\n}\n\ndefined a function sum_sq that returns the sum of the squared elements in a vector x.\n\nVectorization\nOne of the key features of R is that, by default, the vast majority of functions are vectorized. This means that functions, most of the time, are applied element-wise to vectors. For example, given a vector x, we can add 2 to every element in x by simply writing x + 2:\n\nx &lt;- c(1, 2, 3)\nx + 2\n\n[1] 3 4 5\n\n\nIf you want to vectorize a user-defined function, you can use the wrapper Vectorize().\n\n\n\n\n\n\nExercise 1\n\n\n\nTake a look at the help file for the Vectorize() function."
  },
  {
    "objectID": "Files/Workshops_24_25/STATSRFUN1/ANSWERS_Intro_to_Python_+_Data.html",
    "href": "Files/Workshops_24_25/STATSRFUN1/ANSWERS_Intro_to_Python_+_Data.html",
    "title": "",
    "section": "",
    "text": "Creating Variables: - A variable can be set to any data type - You can name a variable anything - Except… it cannot start with a number, or contain periods\n\n# 1) Create a variable with a value of 2, and another one with a value of 3\nx = 2\ny = 3\n\nBasic Math Functions: - Addition: x + y - Subtraction: x - y - Multiplication: x * y - Division: x/y - Exponential: x ** y\n\n# Add, multiply and divide your variables!\nx - y\n\n-1\n\n\n\nx * y\n\n6\n\n\n\nx / y\n\n0.6666666666666666\n\n\nData Types - Integers (‘int’): Whole numbers - Floats (‘float’): Decimals - Strings (‘str’): Sequences of characters surrounded by quotations - Booleans (‘bool’): True/False - Lists (‘list’): Collections of items surrounded by brackets - Dictionaries (‘dict’): Collections of items assigned to labels\n\n# DO NOT REMOVE #\n\nnum1 = 5\nnum2 = 5.0\nnum3 = \"5\"\n\nmy_dict = {\n    'colors': ['red', 'blue', 'green'],\n    'numbers': [1,2,3],\n    'favorite food': 'pizza'\n    }\n\nmy_list = [\"hey\", 35]\n\n# DO NOT REMOVE #\n\n\n# Find the types of each: num1, num2, num3, my_dict and my_list (hint: type())\n\ntype(num1), type(num2), type(num3)\n\nint\n\n\n\n# Create a list of numbers from 5-10 and a dictionary with your favorite\n# foods and favorite drinks\n\nmy_list2 = [5, 6, 7, 8, 9, 10]\n\nmy_dictionary2 = {\n    \"favorite foods\": [\"pizza\", \"pasta\"],\n    \"favorite drinks\": [\"costco oat milk\", \"costco soy milk\"]\n}\n\nmy_list2[0], my_dictionary2[\"favorite foods\"]\n\n(5, ['pizza', 'pasta'])\n\n\nIndexing: retrieving an element from a collection (such as a list of dictionary) - Remember it starts at 0!\n\n# Retrieve the first element of my_list, and the color green from my_dict\n\nmy_list[1], my_dict[\"colors\"][2]\n\n(35, 'green')\n\n\nImporting Data: - Data usually comes in the form of CSV’s (comma seperated values) - We usually look at structured data which is a big table - We can index it like a dictionary!\n\n# Import data here, and show the \"head\"\n\nimport pandas as pd\nbooks = pd.read_csv(\"books.csv\")\n\nbooks.head()\n\n\n\n  \n    \n\n\n\n\n\n\nName\nAuthor\nUser Rating\nReviews\nPrice\nYear\nGenre\n\n\n\n\n0\nAct Like a Lady, Think Like a Man: What Men Re...\nSteve Harvey\n4.6\n5013\n17\n2009\nNon Fiction\n\n\n1\nArguing with Idiots: How to Stop Small Minds a...\nGlenn Beck\n4.6\n798\n5\n2009\nNon Fiction\n\n\n2\nBreaking Dawn (The Twilight Saga, Book 4)\nStephenie Meyer\n4.6\n9769\n13\n2009\nFiction\n\n\n3\nCrazy Love: Overwhelmed by a Relentless God\nFrancis Chan\n4.7\n1542\n14\n2009\nNon Fiction\n\n\n4\nDead And Gone: A Sookie Stackhouse Novel (Sook...\nCharlaine Harris\n4.6\n1541\n4\n2009\nFiction"
  },
  {
    "objectID": "Files/Workshops_24_25/STATSRFUN2/Intro_to_NumPy_and_Pandas.html",
    "href": "Files/Workshops_24_25/STATSRFUN2/Intro_to_NumPy_and_Pandas.html",
    "title": "Getting Started",
    "section": "",
    "text": "Create a copy of the worksheet by going in the top left and selecting File -&gt; Save a copy in Drive\n\n# Import the NumPy and Pandas packages\n\n\nNumPy Arrays\nArrays have the same underlying structure as lists! We can construct them using np.array() and index them the exact same way\nArrays have the same underlying structure as lists! Let us construct them by using np.array()\n\n# Create two NumPy arrays; one from 1-5, and another from 5-10\n\n\n# Retrieve the first element from your arrays\n\n\n# Retrieve the first 3 elements from your arrays\n\nWe can do mathematical operations between arrays such as addition and subtraction!\n\n# Add, subtract and multiply your arrays\n\n\n\n# Find the sum of all elements in your arrays\n\n\n# Use np.linspace() to create an array of values of 0-20, in intervals of 5\n\n\n# Create a 3x3 array of values from 1-9\n\n\n\n# Confirm that your array has dimensions of 3x3\n\n\n# Recreate the same array but with a float data type instead of integer\n\n\n# Confirm that your array have a float data type\n\n\n\nPandas Data Frames\nPandas Data Frames have the same underlying structure as Python dictionaries. We can create them by using pd.DataFrame() and index them the exact same way\n\n# Create a data frame with two columns; one with your three favorite foods and\n# another the values 1-3\n\nImportant Functions:\nExamining your data: - .head(): shows first 5 observations of data\n\n.info(): shows number of rows, columns, blank (“null”) values, and the data types of each variable\n.dtype: shows the underlying data type (integers, floats, etc.)\n\nAnalyzing your data: - .min(): Minimum value of a column - .max(): Maximum value of a column - .mean(): Average value of a column - .median(): Median value of a column - .sum(): Sum of all values in a column - .corr(): Shows correlation between values (NOTE: negative correlation does not mean less correlation, refer to workshop slides!) - .value_counts(): Shows number of observations per value in a column - .nunique(): Shows the amount of unique observations in a column\n\n# Import the bestsellers dataset and view the head\n\n\n# Use .info() to see more details about our dataset\n\n\n# Let's check the type of our dataset!\n\n\n# Retrieve the \"User Rating\" column from our dataset\n\n\n# Find the *data type* of the \"User Rating\" column\n\n\n# Find the average amount of reviews in this dataset\n\n\n\n# Find the cheapest and most expensive book prices in our dataset\n\n\n# Find the names of the cheapest and most expensive books\n\n\n# Which authors have produced the most bestselling books\n\n\n# Find the books that have a user rating less than 4\n\n\n# How many of those books have less than 10,000 reviews?\n\n\n# Find the correlation between User Rating and Reviews"
  },
  {
    "objectID": "Files/Workshops_24_25/STATSRFUN3/Visualizations_Solutions.html",
    "href": "Files/Workshops_24_25/STATSRFUN3/Visualizations_Solutions.html",
    "title": "Data Visualization: Stats R. Fun, #3",
    "section": "",
    "text": "DS Collaborative, Fall 2024. Presented by: Ethan P. Marzban\n## DO NOT EDIT\nimport pandas as pd\nimport numpy as np\nimport altair as alt\nimport scipy.stats as sps\n\nfrom vega_datasets import data\nalt.renderers.enable('html')\n\nRendererRegistry.enable('html')"
  },
  {
    "objectID": "Files/Workshops_24_25/STATSRFUN3/Visualizations_Solutions.html#part-1-basic-visualizations-in-altair",
    "href": "Files/Workshops_24_25/STATSRFUN3/Visualizations_Solutions.html#part-1-basic-visualizations-in-altair",
    "title": "Data Visualization: Stats R. Fun, #3",
    "section": "Part 1: Basic Visualizations in Altair",
    "text": "Part 1: Basic Visualizations in Altair\nAs mentioned during the workshop lecture, today’s workshop will primarily make use of the plotting library “Altair”. You can read more about Altair at https://altair-viz.github.io/.\nLet’s start by making a simple scatterplot. First, we need data! Create a pandas dataframe consisting of two columns, called x and y. Populate the x column with all integers between -3 and 3 (inclusive), and populate the y column with the squares of the corresponding x values. Assign the data frame to a variable (pick the name yourself!) The first few rows of your data frame should look like\n\n\n\nx\ny\n\n\n\n\n-3\n9\n\n\n-2\n4\n\n\n-1\n1\n\n\n\n\n## Replace this cell with your answers\nmy_df = pd.DataFrame({\n    'x': np.arange(-3, 4),\n    'y': np.arange(-3, 4) ** 2\n})\n\nmy_df\n\n\n\n  \n    \n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n-3\n9\n\n\n1\n-2\n4\n\n\n2\n-1\n1\n\n\n3\n0\n0\n\n\n4\n1\n1\n\n\n5\n2\n4\n\n\n6\n3\n9\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nNow that we have data, we can make a graph! Remember that Altair follows the Grammar of Graphics, which posits that visualizations can be broken down into: - Axes: the axes (or axis) of the plot - Geoms: the shapes/objects that comprise the plot (e.g. points for scatterplots, bars for bargraphs and histograms, etc.) - Aesthetics: mappings from the data to the geoms (e.g. coordinates for the points in the scatterplot, length of bars in a bargraph, etc.)\nFurther recall that different variable types best correspond to different plot types. Given that our data frame from above consists of two numerical variables, what type of plot is most appropriate to visualize the data? Once you’ve answered this question, use Altair to generate the appropriate plot. Don’t worry about formatting too much yet.\n\n## Replace this cell with your answers\nalt.Chart(\n    my_df\n).mark_point().encode(\n    x = 'x',\n    y = 'y'\n)\n\n\n\n\n\n\nThough this is a nice enough plot, perhaps we want to highlight the fact that these points lie along a parabola. As such, let’s work toward adding a parabolic curve to our plot that passes through all the points.\nThere are several ways to do this; let’s work through one together. First, let’s focus on generating the graph of the function \\(f(x) = x^2\\) on the interval \\([-3, 3]\\). One way to do this is to create a very fine set of points between \\(-3\\) and \\(3\\), square each, and then create a line graph of the resulting plot. Because the x-values are so close together, the line segments will appear smooth when viewed holistically.\n\n## Replace this cell with your answers\ntemp_df = pd.DataFrame({\n    'x': np.linspace(-3, 3, num = 100),\n    'y':  np.linspace(-3, 3, num = 100) ** 2\n})\n\nparabola1 = alt.Chart(temp_df).encode(x = 'x', y = 'y')\nparabola1.mark_line()\n\n\n\n\n\n\nFinally, we can layer our parabola onto our initial plot. Though we could use the alt.layer() function, we can actually also use the + operator!\n\n## Replace this cell with your answers\n\nscatter1 = alt.Chart(my_df).encode(\n    x = 'x',\n    y = 'y'\n)\n\nscatter1.mark_point(size = 100, filled = True) + parabola1.mark_line()\n\n\n\n\n\n\nFinally, let’s add some formatting to this plot (and a title)!\n\n## Replace this cell with your answers\nplot1 = scatter1.mark_point(size = 100, filled = True) + parabola1.mark_line()\nplot1.properties(title = \"My First Plot\").configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n).configure_title(\n    fontSize = 18\n)"
  },
  {
    "objectID": "Files/Workshops_24_25/STATSRFUN3/Visualizations_Solutions.html#part-2-iowa-electricity-dataset",
    "href": "Files/Workshops_24_25/STATSRFUN3/Visualizations_Solutions.html#part-2-iowa-electricity-dataset",
    "title": "Data Visualization: Stats R. Fun, #3",
    "section": "Part 2: Iowa Electricity Dataset",
    "text": "Part 2: Iowa Electricity Dataset\nLet’s now work with some real data! To avoid having to worry about downloading and uploading data from external sources, we’ll be working with one of the built-in datasets from the vega_datasets module.\nOur specific dataset contains annual net generation of electricity in the state of Iowa by source in thousand megawatthours, between 2001 and 2017. To import and save the dataframe as a variable called iowa, run the following cell:\n\niowa = data.iowa_electricity()\n\nNow, display the first 10 rows of the dataframe. (Yes, this is a bit of a review from previous workshops!)\n\n## Replace this cell with your answers\niowa.head(10)\n\n\n\n  \n    \n\n\n\n\n\n\nyear\nsource\nnet_generation\n\n\n\n\n0\n2001-01-01\nFossil Fuels\n35361\n\n\n1\n2002-01-01\nFossil Fuels\n35991\n\n\n2\n2003-01-01\nFossil Fuels\n36234\n\n\n3\n2004-01-01\nFossil Fuels\n36205\n\n\n4\n2005-01-01\nFossil Fuels\n36883\n\n\n5\n2006-01-01\nFossil Fuels\n37014\n\n\n6\n2007-01-01\nFossil Fuels\n41389\n\n\n7\n2008-01-01\nFossil Fuels\n42734\n\n\n8\n2009-01-01\nFossil Fuels\n38620\n\n\n9\n2010-01-01\nFossil Fuels\n42750\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\nAggregating Across Years; Comparing Across Sectors\nLet’s work toward visualizing the net generation across the three sources (Fossil Fuels, Nuclear Energy, and Renewables), aggregated across all 17 years invluded in the dataset.\nFirst, what type of plot do you think would be most appropriate?\nNow, what we would like to do is aggregate across years, but within each source. We’ll talk more about how to do this in the next workshop (on Data Tidying) - for now, I’ll just mention that we can achieve our result by grouping the dataframe using the pd.groupby() method.\n\naggregate_gen = iowa.groupby('source', as_index=False).sum('net_generation')\naggregate_gen\n\n\n\n  \n    \n\n\n\n\n\n\nsource\nnet_generation\n\n\n\n\n0\nFossil Fuels\n620129\n\n\n1\nNuclear Energy\n80103\n\n\n2\nRenewables\n164220\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nFinally, use this dataframe to generate the desired plot. Adjust the plot to have appropriate font sizes and dimensions - also include a title.\n\n## Replace this cell with your answers\nalt.Chart(\n    aggregate_gen,\n    title = \"Aggregated Generation Across Sectors\"\n).mark_bar().encode(\n    x = 'source',\n    y = 'net_generation'\n).properties(\n    width = 500\n).configure_axis(\n    labelFontSize = 16,\n    titleFontSize = 18\n).configure_title(\n    fontSize = 18\n)\n\n/usr/local/lib/python3.10/dist-packages/altair/utils/core.py:384: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n\n\n\n\n\n\n\n\n\nChanges Over Time\nIt may be interesting to view the annual expenditures changes over time. Produce an appropriate graphic, and interpret.\n\n## Replace this cell with your answers\nalt.Chart(\n    iowa,\n    title = \"Expenditure Over Time\"\n).mark_line(point = True).encode(\n    x = 'year',\n    y = 'net_generation',\n    color = 'source'\n).properties(\n    width = 500\n).configure_axis(\n    labelFontSize = 16,\n    titleFontSize = 18\n).configure_title(\n    fontSize = 18\n)\n\n/usr/local/lib/python3.10/dist-packages/altair/utils/core.py:384: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)"
  },
  {
    "objectID": "Files/Workshops_24_25/STATSRFUN4/Data_Cleaning_ANSWERS.html",
    "href": "Files/Workshops_24_25/STATSRFUN4/Data_Cleaning_ANSWERS.html",
    "title": "Understanding the Data",
    "section": "",
    "text": "# Always remember to install important packages!\nimport pandas as pd\nimport numpy as np\n\n# ------ DO NOT REMOVE ------ #\n\ngrades_df = pd.DataFrame({\n    \"Name\": [\"John\", \"Bob\", \"Charlie\", \"Diana\", \"Edward\", \"Fiona\", \"George\",\n             \"Hannah\", \"Ian\", \"Ian\", \"Liam\", \"Mia\", \"Noah\", \"Olivia\", \"Sophia\"],\n    \"Homework\": [95.0, 82.5, 75.3, 91.2, np.nan, 88.6, 92.3, 85.7, 70.1, 70.1,\n                 78.9, 94.2, np.nan, 86.7, 90.8],\n    \"Exam Score\": [85.5, \"Not Graded\", 65.3, 88.9, 72.2, 92.1, 94.1, 83, 70, 70, 79.7, 91.4, 80.1, 87.4, \"Not Graded\"],\n    \"Office Hours\": [\"Yes\", \"No\", \"Yes\", \"Yes\", \"No\", \"No\", \"Yes\", \"Yes\", \"No\", \"No\",\n                     \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\"],\n    \"Final Grade\": [90.5, 78.2, 65.8, 88.3, 72.4, 91.0, 94.3, 83.5, 68.6, 68.6,\n                    79.7, 90.1, 80.5, 87.4, 82.3]\n})\n\n# ------ DO NOT REMOVE ------ #\n# 1. Use the .info() function to identify data types and missing values\ngrades_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 15 entries, 0 to 14\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   Name          15 non-null     object \n 1   Homework      13 non-null     float64\n 2   Exam Score    15 non-null     object \n 3   Office Hours  15 non-null     object \n 4   Final Grade   15 non-null     float64\ndtypes: float64(2), object(3)\nmemory usage: 732.0+ bytes\n# 2. Return the dataset to observe its structure\ngrades_df\n\n\n\n  \n    \n\n\n\n\n\n\nName\nHomework\nExam Score\nOffice Hours\nFinal Grade\n\n\n\n\n0\nJohn\n95.0\n85.5\nYes\n90.5\n\n\n1\nBob\n82.5\nNot Graded\nNo\n78.2\n\n\n2\nCharlie\n75.3\n65.3\nYes\n65.8\n\n\n3\nDiana\n91.2\n88.9\nYes\n88.3\n\n\n4\nEdward\nNaN\n72.2\nNo\n72.4\n\n\n5\nFiona\n88.6\n92.1\nNo\n91.0\n\n\n6\nGeorge\n92.3\n94.1\nYes\n94.3\n\n\n7\nHannah\n85.7\n83\nYes\n83.5\n\n\n8\nIan\n70.1\n70\nNo\n68.6\n\n\n9\nIan\n70.1\n70\nNo\n68.6\n\n\n10\nLiam\n78.9\n79.7\nYes\n79.7\n\n\n11\nMia\n94.2\n91.4\nNo\n90.1\n\n\n12\nNoah\nNaN\n80.1\nYes\n80.5\n\n\n13\nOlivia\n86.7\n87.4\nNo\n87.4\n\n\n14\nSophia\n90.8\nNot Graded\nYes\n82.3\nOur data is a bit messy! We can see missing values in the “Homework” and “Exam Score” volumns as well as the categorical data in the “Office Hours” column."
  },
  {
    "objectID": "Files/Workshops_24_25/STATSRFUN4/Data_Cleaning_ANSWERS.html#duplicate-values",
    "href": "Files/Workshops_24_25/STATSRFUN4/Data_Cleaning_ANSWERS.html#duplicate-values",
    "title": "Understanding the Data",
    "section": "Duplicate Values",
    "text": "Duplicate Values\n\n# 3. Check how many duplicate rows are in the dataset (if any)\ngrades_df.duplicated().sum()\n\n1\n\n\n\n# 4. Drop duplicate rows (if any)\ngrades_df.drop_duplicates(inplace=True)\ngrades_df\n\n\n\n  \n    \n\n\n\n\n\n\nName\nHomework\nExam Score\nOffice Hours\nFinal Grade\n\n\n\n\n0\nJohn\n95.0\n85.5\nYes\n90.5\n\n\n1\nBob\n82.5\nNot Graded\nNo\n78.2\n\n\n2\nCharlie\n75.3\n65.3\nYes\n65.8\n\n\n3\nDiana\n91.2\n88.9\nYes\n88.3\n\n\n4\nEdward\nNaN\n72.2\nNo\n72.4\n\n\n5\nFiona\n88.6\n92.1\nNo\n91.0\n\n\n6\nGeorge\n92.3\n94.1\nYes\n94.3\n\n\n7\nHannah\n85.7\n83\nYes\n83.5\n\n\n8\nIan\n70.1\n70\nNo\n68.6\n\n\n10\nLiam\n78.9\n79.7\nYes\n79.7\n\n\n11\nMia\n94.2\n91.4\nNo\n90.1\n\n\n12\nNoah\nNaN\n80.1\nYes\n80.5\n\n\n13\nOlivia\n86.7\n87.4\nNo\n87.4\n\n\n14\nSophia\n90.8\nNot Graded\nYes\n82.3"
  },
  {
    "objectID": "Files/Workshops_24_25/STATSRFUN4/Data_Cleaning_ANSWERS.html#missing-values",
    "href": "Files/Workshops_24_25/STATSRFUN4/Data_Cleaning_ANSWERS.html#missing-values",
    "title": "Understanding the Data",
    "section": "Missing Values",
    "text": "Missing Values\nThe missing values in the “Homework” column are represented by “NaN” which can be handled easier.\n\n# 5. Replace missing values in the \"Homework\" column with the average\n# homework score\ngrades_df[\"Homework\"].fillna(grades_df[\"Homework\"].mean().round(2),\n                             inplace=True)\ngrades_df\n\nFutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  grades_df[\"Homework\"].fillna(grades_df[\"Homework\"].mean().round(2),\n\n\n\n\n  \n    \n\n\n\n\n\n\nName\nHomework\nExam Score\nOffice Hours\nFinal Grade\n\n\n\n\n0\nJohn\n95.00\n85.5\nYes\n90.5\n\n\n1\nBob\n82.50\nNot Graded\nNo\n78.2\n\n\n2\nCharlie\n75.30\n65.3\nYes\n65.8\n\n\n3\nDiana\n91.20\n88.9\nYes\n88.3\n\n\n4\nEdward\n85.94\n72.2\nNo\n72.4\n\n\n5\nFiona\n88.60\n92.1\nNo\n91.0\n\n\n6\nGeorge\n92.30\n94.1\nYes\n94.3\n\n\n7\nHannah\n85.70\n83\nYes\n83.5\n\n\n8\nIan\n70.10\n70\nNo\n68.6\n\n\n10\nLiam\n78.90\n79.7\nYes\n79.7\n\n\n11\nMia\n94.20\n91.4\nNo\n90.1\n\n\n12\nNoah\n85.94\n80.1\nYes\n80.5\n\n\n13\nOlivia\n86.70\n87.4\nNo\n87.4\n\n\n14\nSophia\n90.80\nNot Graded\nYes\n82.3\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nNext, we must also handle the missing values in the “Exam Score” column. Since it is not in the form of “None” or “NaN”, we will need to handle it in a different way.\n\n# 6. Replace missing values in the \"Exam Score\" column with the average\n# exam score\ngrades_df[\"Exam Score\"] = grades_df[\"Exam Score\"].replace(\"Not Graded\", np.nan)\n\ngrades_df.fillna(grades_df[\"Exam Score\"].mean(), inplace=True)\n\ngrades_df\n\n\n\n  \n    \n\n\n\n\n\n\nName\nHomework\nExam Score\nOffice Hours\nFinal Grade\n\n\n\n\n0\nJohn\n95.00\n85.500\nYes\n90.5\n\n\n1\nBob\n82.50\n82.475\nNo\n78.2\n\n\n2\nCharlie\n75.30\n65.300\nYes\n65.8\n\n\n3\nDiana\n91.20\n88.900\nYes\n88.3\n\n\n4\nEdward\n85.94\n72.200\nNo\n72.4\n\n\n5\nFiona\n88.60\n92.100\nNo\n91.0\n\n\n6\nGeorge\n92.30\n94.100\nYes\n94.3\n\n\n7\nHannah\n85.70\n83.000\nYes\n83.5\n\n\n8\nIan\n70.10\n70.000\nNo\n68.6\n\n\n10\nLiam\n78.90\n79.700\nYes\n79.7\n\n\n11\nMia\n94.20\n91.400\nNo\n90.1\n\n\n12\nNoah\n85.94\n80.100\nYes\n80.5\n\n\n13\nOlivia\n86.70\n87.400\nNo\n87.4\n\n\n14\nSophia\n90.80\n82.475\nYes\n82.3"
  },
  {
    "objectID": "Files/Workshops_24_25/STATSRFUN4/Data_Cleaning_ANSWERS.html#factoring-categorical-data",
    "href": "Files/Workshops_24_25/STATSRFUN4/Data_Cleaning_ANSWERS.html#factoring-categorical-data",
    "title": "Understanding the Data",
    "section": "Factoring Categorical Data",
    "text": "Factoring Categorical Data\n\n# 7. Turn the categorical data in the \"Office Hours\" columns to numerical\ngrades_df[\"Office Hours\"] = np.where(grades_df[\"Office Hours\"] == \"Yes\", 1, 0)\ngrades_df\n\n\n\n  \n    \n\n\n\n\n\n\nName\nHomework\nExam Score\nOffice Hours\nFinal Grade\n\n\n\n\n0\nJohn\n95.00\n85.500\n1\n90.5\n\n\n1\nBob\n82.50\n82.475\n0\n78.2\n\n\n2\nCharlie\n75.30\n65.300\n1\n65.8\n\n\n3\nDiana\n91.20\n88.900\n1\n88.3\n\n\n4\nEdward\n85.94\n72.200\n0\n72.4\n\n\n5\nFiona\n88.60\n92.100\n0\n91.0\n\n\n6\nGeorge\n92.30\n94.100\n1\n94.3\n\n\n7\nHannah\n85.70\n83.000\n1\n83.5\n\n\n8\nIan\n70.10\n70.000\n0\n68.6\n\n\n10\nLiam\n78.90\n79.700\n1\n79.7\n\n\n11\nMia\n94.20\n91.400\n0\n90.1\n\n\n12\nNoah\n85.94\n80.100\n1\n80.5\n\n\n13\nOlivia\n86.70\n87.400\n0\n87.4\n\n\n14\nSophia\n90.80\n82.475\n1\n82.3"
  },
  {
    "objectID": "Files/Workshops_24_25/STATSRFUN4/Data_Cleaning_ANSWERS.html#removing-columns",
    "href": "Files/Workshops_24_25/STATSRFUN4/Data_Cleaning_ANSWERS.html#removing-columns",
    "title": "Understanding the Data",
    "section": "Removing Columns",
    "text": "Removing Columns\n\n# 8. Remove the \"Name\" column from our dataset\ngrades_df.drop(columns=\"Name\", inplace=True)\ngrades_df\n\n\n\n  \n    \n\n\n\n\n\n\nHomework\nExam Score\nOffice Hours\nFinal Grade\n\n\n\n\n0\n95.00\n85.500\n1\n90.5\n\n\n1\n82.50\n82.475\n0\n78.2\n\n\n2\n75.30\n65.300\n1\n65.8\n\n\n3\n91.20\n88.900\n1\n88.3\n\n\n4\n85.94\n72.200\n0\n72.4\n\n\n5\n88.60\n92.100\n0\n91.0\n\n\n6\n92.30\n94.100\n1\n94.3\n\n\n7\n85.70\n83.000\n1\n83.5\n\n\n8\n70.10\n70.000\n0\n68.6\n\n\n10\n78.90\n79.700\n1\n79.7\n\n\n11\n94.20\n91.400\n0\n90.1\n\n\n12\n85.94\n80.100\n1\n80.5\n\n\n13\n86.70\n87.400\n0\n87.4\n\n\n14\n90.80\n82.475\n1\n82.3"
  },
  {
    "objectID": "index.html#upcoming-events",
    "href": "index.html#upcoming-events",
    "title": "",
    "section": "Upcoming Events",
    "text": "Upcoming Events\n\n\n\nEvent\nDate/Time\nLocation\n\n\n\n\nTBA\nTBA\nTBA"
  },
  {
    "objectID": "Files/Workshops_24_25/STATSRFUN4/Data_Cleaning_Workshop.html",
    "href": "Files/Workshops_24_25/STATSRFUN4/Data_Cleaning_Workshop.html",
    "title": "Understanding the Data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\n# ------ DO NOT REMOVE ------ #\n\ngrades_df = pd.DataFrame({\n    \"Name\": [\"John\", \"Bob\", \"Charlie\", \"Diana\", \"Edward\", \"Fiona\", \"George\",\n             \"Hannah\", \"Ian\", \"Ian\", \"Liam\", \"Mia\", \"Noah\", \"Olivia\", \"Sophia\"],\n    \"Homework\": [95.0, 82.5, 75.3, 91.2, np.nan, 88.6, 92.3, 85.7, 70.1, 70.1,\n                 78.9, 94.2, np.nan, 86.7, 90.8],\n    \"Exam Score\": [85.5, \"Not Graded\", 65.3, 88.9, 72.2, 92.1, 94.1, 83, 70, 70, 79.7, 91.4, 80.1, 87.4, \"Not Graded\"],\n    \"Office Hours\": [\"Yes\", \"No\", \"Yes\", \"Yes\", \"No\", \"No\", \"Yes\", \"Yes\", \"No\", \"No\",\n                     \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\"],\n    \"Final Grade\": [90.5, 78.2, 65.8, 88.3, 72.4, 91.0, 94.3, 83.5, 68.6, 68.6,\n                    79.7, 90.1, 80.5, 87.4, 82.3]\n})\n\n# ------ DO NOT REMOVE ------ #\n# 1. Use the .info() function to identify data types and missing values\n# 2. Return the dataset to observe its structure\n\n\n\n  \n    \n\n\n\n\n\n\nName\nHomework\nExam Score\nOffice Hours\nFinal Grade\n\n\n\n\n0\nJohn\n95.0\n85.5\nYes\n90.5\n\n\n1\nBob\n82.5\nNot Graded\nNo\n78.2\n\n\n2\nCharlie\n75.3\n65.3\nYes\n65.8\n\n\n3\nDiana\n91.2\n88.9\nYes\n88.3\n\n\n4\nEdward\nNaN\n72.2\nNo\n72.4\n\n\n5\nFiona\n88.6\n92.1\nNo\n91.0\n\n\n6\nGeorge\n92.3\n94.1\nYes\n94.3\n\n\n7\nHannah\n85.7\n83\nYes\n83.5\n\n\n8\nIan\n70.1\n70\nNo\n68.6\n\n\n9\nIan\n70.1\n70\nNo\n68.6\n\n\n10\nLiam\n78.9\n79.7\nYes\n79.7\n\n\n11\nMia\n94.2\n91.4\nNo\n90.1\n\n\n12\nNoah\nNaN\n80.1\nYes\n80.5\n\n\n13\nOlivia\n86.7\n87.4\nNo\n87.4\n\n\n14\nSophia\n90.8\nNot Graded\nYes\n82.3\nOur data is a bit messy! We can see missing values in the “Homework” and “Exam Score” volumns as well as the categorical data in the “Office Hours” column."
  },
  {
    "objectID": "Files/Workshops_24_25/STATSRFUN4/Data_Cleaning_Workshop.html#duplicate-values",
    "href": "Files/Workshops_24_25/STATSRFUN4/Data_Cleaning_Workshop.html#duplicate-values",
    "title": "Understanding the Data",
    "section": "Duplicate Values",
    "text": "Duplicate Values\n\n# 3. Check how many duplicate rows are in the dataset (if any)\n\n1\n\n\n\n# 4. Drop duplicate rows (if any)"
  },
  {
    "objectID": "Files/Workshops_24_25/STATSRFUN4/Data_Cleaning_Workshop.html#missing-values",
    "href": "Files/Workshops_24_25/STATSRFUN4/Data_Cleaning_Workshop.html#missing-values",
    "title": "Understanding the Data",
    "section": "Missing Values",
    "text": "Missing Values\nThe missing values in the “Homework” column are represented by “NaN” which can be handled easier.\n\n# 5. Replace missing values in the \"Homework\" column with the average\n# homework score\n\nFutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  grades_df[\"Homework\"].fillna(grades_df[\"Homework\"].mean().round(2), inplace=True)\n\n\nNext, we must also handle the missing values in the “Exam Score” column. Since it is not in the form of “None” or “NaN”, we will need to handle it in a different way.\n\n# 6. Replace missing values in the \"Exam Score\" column with the average\n# exam score\n\nFutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  grades_df[\"Exam Score\"] = grades_df[\"Exam Score\"].replace(\"Not Graded\", np.nan)"
  },
  {
    "objectID": "Files/Workshops_24_25/STATSRFUN4/Data_Cleaning_Workshop.html#factoring-categorical-data",
    "href": "Files/Workshops_24_25/STATSRFUN4/Data_Cleaning_Workshop.html#factoring-categorical-data",
    "title": "Understanding the Data",
    "section": "Factoring Categorical Data",
    "text": "Factoring Categorical Data\n\n# 7. Turn the categorical data in the \"Office Hours\" columns to numerical\n\n\n\n  \n    \n\n\n\n\n\n\nName\nHomework\nExam Score\nOffice Hours\nFinal Grade\n\n\n\n\n0\nJohn\n95.00\n85.500\n1\n90.5\n\n\n1\nBob\n82.50\n82.475\n0\n78.2\n\n\n2\nCharlie\n75.30\n65.300\n1\n65.8\n\n\n3\nDiana\n91.20\n88.900\n1\n88.3\n\n\n4\nEdward\n85.94\n72.200\n0\n72.4\n\n\n5\nFiona\n88.60\n92.100\n0\n91.0\n\n\n6\nGeorge\n92.30\n94.100\n1\n94.3\n\n\n7\nHannah\n85.70\n83.000\n1\n83.5\n\n\n8\nIan\n70.10\n70.000\n0\n68.6\n\n\n10\nLiam\n78.90\n79.700\n1\n79.7\n\n\n11\nMia\n94.20\n91.400\n0\n90.1\n\n\n12\nNoah\n85.94\n80.100\n1\n80.5\n\n\n13\nOlivia\n86.70\n87.400\n0\n87.4\n\n\n14\nSophia\n90.80\n82.475\n1\n82.3"
  },
  {
    "objectID": "Files/Workshops_24_25/STATSRFUN4/Data_Cleaning_Workshop.html#removing-columns",
    "href": "Files/Workshops_24_25/STATSRFUN4/Data_Cleaning_Workshop.html#removing-columns",
    "title": "Understanding the Data",
    "section": "Removing Columns",
    "text": "Removing Columns\n\n# 8. Remove the \"Name\" column from our dataset\nnew_grades_df = grades_df.drop(columns=\"Name\", inplace=True)\ngrades_df\n\n\n\n  \n    \n\n\n\n\n\n\nHomework\nExam Score\nOffice Hours\nFinal Grade\n\n\n\n\n0\n95.00\n85.500\n1\n90.5\n\n\n1\n82.50\n82.475\n0\n78.2\n\n\n2\n75.30\n65.300\n1\n65.8\n\n\n3\n91.20\n88.900\n1\n88.3\n\n\n4\n85.94\n72.200\n0\n72.4\n\n\n5\n88.60\n92.100\n0\n91.0\n\n\n6\n92.30\n94.100\n1\n94.3\n\n\n7\n85.70\n83.000\n1\n83.5\n\n\n8\n70.10\n70.000\n0\n68.6\n\n\n10\n78.90\n79.700\n1\n79.7\n\n\n11\n94.20\n91.400\n0\n90.1\n\n\n12\n85.94\n80.100\n1\n80.5\n\n\n13\n86.70\n87.400\n0\n87.4\n\n\n14\n90.80\n82.475\n1\n82.3"
  },
  {
    "objectID": "Files/Workshops_24_25/STATSRFUN2/ANSWERS_Intro_to_NumPy_and_Pandas.html",
    "href": "Files/Workshops_24_25/STATSRFUN2/ANSWERS_Intro_to_NumPy_and_Pandas.html",
    "title": "Getting Started",
    "section": "",
    "text": "Create a copy of the worksheet by going in the top left and selecting File -&gt; Save a copy in Drive\n\n# Import NumPy and Pandas\nimport pandas as pd\nimport numpy as np\n\n\nmovies = pd.DataFrame({\n    \"Movie\": [\"Cars 2\", \"The Exorcist\", \"Click\"],\n    \"Rating\": [4.2, 4.1, 4.7],\n    \"Genre\": [\"Family\", \"Horror\", \"Comedy\"]\n})\n\nmovies.iloc[0]\n\n\n\n\n\n\n\n\n\n0\n\n\n\n\nMovie\nCars 2\n\n\nRating\n4.2\n\n\nGenre\nFamily\n\n\n\n\ndtype: object\n\n\n\n\nNumPy Arrays\nArrays have the same underlying structure as lists! We can construct them using np.array() and index them the exact same way\n\n# Create two NumPy arrays; one from 1-5, and another from 5-10\n\nmy_array1 = np.array([1,2,3,4,5])\nmy_array2 = np.array([6,7,8,9,10])\n\n\n# Retrieve the first element from your arrays\nmy_array1[0], my_array2[0]\n\n(1, 6)\n\n\nWe can do mathematical operations between arrays such as addition and subtraction!\n\n# Add, subtract and multiply your arrays\nmy_array1 + my_array2\n\narray([-5, -5, -5, -5, -5])\n\n\n\nmy_array1 - my_array2\n\narray([-5, -5, -5, -5, -5])\n\n\n\nmy_array1 * my_array2\n\narray([ 6, 14, 24, 36, 50])\n\n\n\n# Retrieve the first 3 elements from your arrays\nmy_array1[0:3]\n\narray([1, 2, 3])\n\n\n\n# Find the sum of all elements in your arrays\nmy_array2.sum()\n\n40\n\n\n\n# Use np.linspace() to create an array of values of 0-20, in intervals of 5\nfives = np.linspace(start=0, stop=20, num=5)\nfives\n\narray([ 0.,  5., 10., 15., 20.])\n\n\n\n# Create a 3x3 array of values from 1-9\nbig_array = np.array([[1,2,3],\n                     [4,5,6],\n                     [7,8,9]])\nbig_array\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\n\n# Confirm that your array has dimensions of 3x3\nbig_array.shape\n\n(3, 3)\n\n\n\n# Recreate the same array but with a float data type instead of integer\nbig_array2 = np.array([[1,2,3],\n                     [4,5,6],\n                     [7,8,9]], dtype=\"float\")\n\nbig_array2\n\narray([[1., 2., 3.],\n       [4., 5., 6.],\n       [7., 8., 9.]])\n\n\n\n# Confirm that your array have a float data type\nbig_array2.dtype\n\ndtype('float64')\n\n\n\n\nPandas Data Frames\nPandas Data Frames have the same underlying structure as Python dictionaries. We can create them by using pd.DataFrame() and index them the exact same way\n\n# Create a data frame with two columns; one with your three favorite foods and\n# another the values 1-3\n\nmy_df = pd.DataFrame({\n    \"favorite foods\": [\"pizza\", \"chicken bake\", \"melatonin gummies\"],\n    \"numbers\": [1,2,3]\n})\n\nmy_df\n\n\n\n  \n    \n\n\n\n\n\n\nfavorite foods\nnumbers\n\n\n\n\n0\npizza\n1\n\n\n1\nchicken bake\n2\n\n\n2\nmelatonin gummies\n3\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\n# Retrieve your favorite foods from your data frame\nmy_df[\"favorite foods\"]\n\n\n\n\n\n\n\n\n\nfavorite foods\n\n\n\n\n0\npizza\n\n\n1\nchicken bake\n\n\n2\nmelatonin gummies\n\n\n\n\ndtype: object\n\n\n\n\n# Retrieve the first row of your data frame\nmy_df.iloc[0]\n\n\n\n\n\n\n\n\n\n0\n\n\n\n\nfavorite foods\npizza\n\n\nnumbers\n1\n\n\n\n\ndtype: object\n\n\n\nImportant Functions:\nExamining your data: - .head(): shows first 5 observations of data\n\n.info(): shows number of rows, columns, blank (“null”) values, and the data types of each variable\n.dtype: shows the underlying data type (integers, floats, etc.)\n\nAnalyzing your data: - .min(): Minimum value of a column - .max(): Maximum value of a column - .mean(): Average value of a column - .median(): Median value of a column - .sum(): Sum of all values in a column - .corr(): Shows correlation between values (NOTE: negative correlation does not mean less correlation, refer to workshop slides!) - .value_counts(): Shows number of observations per value in a column - .nunique(): Shows the amount of unique observations in a column\n\n# Import the bestsellers dataset and view the head\nbooks = pd.read_csv(\"bestsellers with categories.csv\")\nbooks.head()\n\n\n\n  \n    \n\n\n\n\n\n\nName\nAuthor\nUser Rating\nReviews\nPrice\nYear\nGenre\n\n\n\n\n0\n10-Day Green Smoothie Cleanse\nJJ Smith\n4.7\n17350\n8\n2016\nNon Fiction\n\n\n1\n11/22/63: A Novel\nStephen King\n4.6\n2052\n22\n2011\nFiction\n\n\n2\n12 Rules for Life: An Antidote to Chaos\nJordan B. Peterson\n4.7\n18979\n15\n2018\nNon Fiction\n\n\n3\n1984 (Signet Classics)\nGeorge Orwell\n4.7\n21424\n6\n2017\nFiction\n\n\n4\n5,000 Awesome Facts (About Everything!) (Natio...\nNational Geographic Kids\n4.8\n7665\n12\n2019\nNon Fiction\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\nbooks.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 550 entries, 0 to 549\nData columns (total 7 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Name         550 non-null    object \n 1   Author       550 non-null    object \n 2   User Rating  550 non-null    float64\n 3   Reviews      550 non-null    int64  \n 4   Price        550 non-null    int64  \n 5   Year         550 non-null    int64  \n 6   Genre        550 non-null    object \ndtypes: float64(1), int64(3), object(3)\nmemory usage: 30.2+ KB\n\n\n\n# Let's check the type of our dataset!\ntype(books)\n\n\n    pandas.core.frame.DataFramedef __init__(data=None, index: Axes | None=None, columns: Axes | None=None, dtype: Dtype | None=None, copy: bool | None=None) -&gt; None/usr/local/lib/python3.10/dist-packages/pandas/core/frame.pyTwo-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns).\nArithmetic operations align on both row and column labels. Can be\nthought of as a dict-like container for Series objects. The primary\npandas data structure.\n\nParameters\n----------\ndata : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n    data is a dict, column order follows insertion-order. If a dict contains Series\n    which have an index defined, it is aligned by its index. This alignment also\n    occurs if data is a Series or a DataFrame itself. Alignment is done on\n    Series/DataFrame inputs.\n\n    If data is a list of dicts, column order follows insertion-order.\n\nindex : Index or array-like\n    Index to use for resulting frame. Will default to RangeIndex if\n    no indexing information part of input data and no index provided.\ncolumns : Index or array-like\n    Column labels to use for resulting frame when data does not have them,\n    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n    will perform column selection instead.\ndtype : dtype, default None\n    Data type to force. Only a single dtype is allowed. If None, infer.\ncopy : bool or None, default None\n    Copy data from inputs.\n    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n    or 2d ndarray input, the default of None behaves like ``copy=False``.\n    If data is a dict containing one or more Series (possibly of different dtypes),\n    ``copy=False`` will ensure that these inputs are not copied.\n\n    .. versionchanged:: 1.3.0\n\nSee Also\n--------\nDataFrame.from_records : Constructor from tuples, also record arrays.\nDataFrame.from_dict : From dicts of Series, arrays, or dicts.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_table : Read general delimited file into DataFrame.\nread_clipboard : Read text from clipboard into DataFrame.\n\nNotes\n-----\nPlease reference the :ref:`User Guide &lt;basics.dataframe&gt;` for more information.\n\nExamples\n--------\nConstructing DataFrame from a dictionary.\n\n&gt;&gt;&gt; d = {'col1': [1, 2], 'col2': [3, 4]}\n&gt;&gt;&gt; df = pd.DataFrame(data=d)\n&gt;&gt;&gt; df\n   col1  col2\n0     1     3\n1     2     4\n\nNotice that the inferred dtype is int64.\n\n&gt;&gt;&gt; df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nTo enforce a single dtype:\n\n&gt;&gt;&gt; df = pd.DataFrame(data=d, dtype=np.int8)\n&gt;&gt;&gt; df.dtypes\ncol1    int8\ncol2    int8\ndtype: object\n\nConstructing DataFrame from a dictionary including Series:\n\n&gt;&gt;&gt; d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n&gt;&gt;&gt; pd.DataFrame(data=d, index=[0, 1, 2, 3])\n   col1  col2\n0     0   NaN\n1     1   NaN\n2     2   2.0\n3     3   3.0\n\nConstructing DataFrame from numpy ndarray:\n\n&gt;&gt;&gt; df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n...                    columns=['a', 'b', 'c'])\n&gt;&gt;&gt; df2\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\n&gt;&gt;&gt; data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n&gt;&gt;&gt; df3 = pd.DataFrame(data, columns=['c', 'a'])\n...\n&gt;&gt;&gt; df3\n   c  a\n0  3  1\n1  6  4\n2  9  7\n\nConstructing DataFrame from dataclass:\n\n&gt;&gt;&gt; from dataclasses import make_dataclass\n&gt;&gt;&gt; Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n&gt;&gt;&gt; pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n   x  y\n0  0  0\n1  0  3\n2  2  3\n\nConstructing DataFrame from Series/DataFrame:\n\n&gt;&gt;&gt; ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n&gt;&gt;&gt; df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n&gt;&gt;&gt; df\n   0\na  1\nc  3\n\n&gt;&gt;&gt; df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n&gt;&gt;&gt; df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n&gt;&gt;&gt; df2\n   x\na  1\nc  3\n      \n      \n\n\n\n# Retrieve the \"User Rating\" column from our dataset\nbooks[\"User Rating\"].head()\n\n\n\n\n\n\n\n\n\nUser Rating\n\n\n\n\n0\n4.7\n\n\n1\n4.6\n\n\n2\n4.7\n\n\n3\n4.7\n\n\n4\n4.8\n\n\n\n\ndtype: float64\n\n\n\n\n# Find the *data type* of the \"User Rating\" column\nbooks[\"User Rating\"].dtype\n\ndtype('float64')\n\n\n\n# Find the average amount of reviews in this dataset\nbooks[\"Reviews\"].mean()\n\n11953.281818181818\n\n\n\n# Find the cheapest and most expensive book prices in our dataset\n\nbooks[\"Price\"].max(), books[\"Price\"].min()\n\n(105, 0)\n\n\n\n# Find the names of the cheapest and most expensive books\nbooks[books[\"Price\"] == books[\"Price\"].max()][\"Name\"]\n\n\n\n\n\n\n\n\n\nName\n\n\n\n\n69\nDiagnostic and Statistical Manual of Mental Di...\n\n\n70\nDiagnostic and Statistical Manual of Mental Di...\n\n\n\n\ndtype: object\n\n\n\n\nbooks[books[\"Price\"] == books[\"Price\"].min()][\"Name\"]\n\n\n\n\n\n\n\n\n\nName\n\n\n\n\n42\nCabin Fever (Diary of a Wimpy Kid, Book 6)\n\n\n71\nDiary of a Wimpy Kid: Hard Luck, Book 8\n\n\n116\nFrozen (Little Golden Book)\n\n\n193\nJOURNEY TO THE ICE P\n\n\n219\nLittle Blue Truck\n\n\n358\nThe Constitution of the United States\n\n\n381\nThe Getaway\n\n\n461\nThe Short Second Life of Bree Tanner: An Eclip...\n\n\n505\nTo Kill a Mockingbird\n\n\n506\nTo Kill a Mockingbird\n\n\n507\nTo Kill a Mockingbird\n\n\n508\nTo Kill a Mockingbird\n\n\n\n\ndtype: object\n\n\n\n\n# Which authors have produced the most bestselling books\nbooks[\"Author\"].value_counts().head()\n\n\n\n\n\n\n\n\n\ncount\n\n\nAuthor\n\n\n\n\n\nJeff Kinney\n12\n\n\nGary Chapman\n11\n\n\nRick Riordan\n11\n\n\nSuzanne Collins\n11\n\n\nAmerican Psychological Association\n10\n\n\n\n\ndtype: int64\n\n\n\n\n# Find the books that have a user rating less than 4\nbooks[books[\"User Rating\"] &lt; 4]\n\n\n\n  \n    \n\n\n\n\n\n\nName\nAuthor\nUser Rating\nReviews\nPrice\nYear\nGenre\n\n\n\n\n22\nAllegiant\nVeronica Roth\n3.9\n6310\n13\n2013\nFiction\n\n\n106\nFifty Shades of Grey: Book One of the Fifty Sh...\nE L James\n3.8\n47265\n14\n2012\nFiction\n\n\n107\nFifty Shades of Grey: Book One of the Fifty Sh...\nE L James\n3.8\n47265\n14\n2013\nFiction\n\n\n132\nGo Set a Watchman: A Novel\nHarper Lee\n3.6\n14982\n19\n2015\nFiction\n\n\n353\nThe Casual Vacancy\nJ.K. Rowling\n3.3\n9372\n12\n2012\nFiction\n\n\n392\nThe Goldfinch: A Novel (Pulitzer Prize for Fic...\nDonna Tartt\n3.9\n33844\n20\n2013\nFiction\n\n\n393\nThe Goldfinch: A Novel (Pulitzer Prize for Fic...\nDonna Tartt\n3.9\n33844\n20\n2014\nFiction\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\n# How many of those books have less than 10,000 reviews?\nbooks[(books[\"User Rating\"] &lt; 4) & (books[\"Reviews\"] &lt; 10000)]\n\n\n\n  \n    \n\n\n\n\n\n\nName\nAuthor\nUser Rating\nReviews\nPrice\nYear\nGenre\n\n\n\n\n22\nAllegiant\nVeronica Roth\n3.9\n6310\n13\n2013\nFiction\n\n\n353\nThe Casual Vacancy\nJ.K. Rowling\n3.3\n9372\n12\n2012\nFiction\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\n# Find the correlation between User Rating and Reviews\nbooks[[\"User Rating\", \"Reviews\"]].corr()\n\n\n\n  \n    \n\n\n\n\n\n\nUser Rating\nReviews\n\n\n\n\nUser Rating\n1.000000\n-0.001729\n\n\nReviews\n-0.001729\n1.000000"
  },
  {
    "objectID": "Files/Workshops_24_25/STATSRFUN1/Intro_to_Python_+_Data.html",
    "href": "Files/Workshops_24_25/STATSRFUN1/Intro_to_Python_+_Data.html",
    "title": "",
    "section": "",
    "text": "Creating Variables: - A variable can be set to any data type - You can name a variable anything - Except… it cannot start with a number, or contain periods\n\n# 1) Create a variable with a value of 2, and another one with a value of 3\n\nBasic Math Functions: - Addition: x + y - Subtraction: x - y - Multiplication: x * y - Division: x/y - Exponential: x ** y\n\n# Add, multiply and divide your variables!\n\nData Types - Integers (‘int’): Whole numbers - Floats (‘float’): Decimals - Strings (‘str’): Sequences of characters surrounded by quotations - Booleans (‘bool’): True/False - Lists (‘list’): Collections of items surrounded by brackets - Dictionaries (‘dict’): Collections of items assigned to labels\n\n# DO NOT REMOVE #\n\nnum1 = 5\nnum2 = 5.0\nnum3 = \"5\"\n\nmy_dict = {\n    'colors': ['red', 'blue', 'green'],\n    'numbers': [1,2,3],\n    'favorite food': 'pizza'\n    }\n\nmy_list = [\"hey\", 35]\n\n# DO NOT REMOVE #\n\n\n# Find the types of each: num1, num2, num3, my_dict and my_list (hint: type())\n\n\n# Create a list of numbers from 5-10 and a dictionary with your favorite\n# foods and favorite drinks\n\nIndexing: retrieving an element from a collection (such as a list of dictionary) - Remember it starts at 0!\n\n# Retrieve the first element of my_list, and the color green from my_dict\n\nImporting Data: - Data usually comes in the form of CSV’s (comma seperated values) - We usually look at structured data which is a big table - We can index it like a dictionary!\n\n# Import data here, and show the \"head\""
  },
  {
    "objectID": "Files/R_Labs/L01_Basics/R_l01.html",
    "href": "Files/R_Labs/L01_Basics/R_l01.html",
    "title": "Programming in R",
    "section": "",
    "text": "Please make sure you have downloaded both R and RStudio, as outlined in the previous lab."
  },
  {
    "objectID": "Files/R_Labs/L01_Basics/R_l01.html#prerequisites",
    "href": "Files/R_Labs/L01_Basics/R_l01.html#prerequisites",
    "title": "Programming in R",
    "section": "",
    "text": "Please make sure you have downloaded both R and RStudio, as outlined in the previous lab."
  },
  {
    "objectID": "Files/R_Labs/L01_Basics/R_l01.html#what-is-programming",
    "href": "Files/R_Labs/L01_Basics/R_l01.html#what-is-programming",
    "title": "Programming in R",
    "section": "What Is Programming?",
    "text": "What Is Programming?\nComputers are an incredibly useful tool in a Data Scientist’s arsenal. They are, however, also incredibly complex and can be difficult to communicate with. As such, programming languages are used to help us communicate with computers and provide them with instructions on what we want them to do. There are several different programming languages: the two most often used in Data Science circles are R and Python, though other popular languages include Julia, MatLab, and C+.\nComputer programs can be written in a variety of different environments and editors; for example, Python can be written in Jupyter Notebooks, VS Code, and several others. R is most commonly run in RStudio, which can be downloaded for free (along with the programming language R). Please see the previous lab for further instructions on how to download R and RStudio."
  },
  {
    "objectID": "Files/R_Labs/L01_Basics/R_l01.html#programming-quick-start",
    "href": "Files/R_Labs/L01_Basics/R_l01.html#programming-quick-start",
    "title": "Programming in R",
    "section": "Programming Quick Start",
    "text": "Programming Quick Start\nThere is a reason we use the word “language” to describe programming languages- that is because they function quite like a human language. This means, among other things, that they each have their own syntax (i.e. set of grammar rules).\nPrograms are made up of expressions, like 2 + 2. We evaluate expressions by running (or executing) them in a programming language. Expressions are like the sentences of programming- they contain complex pieces of information that are conveyed between the user and the computer.\nMuch like sentences in other languages, expressions must obey a rigid syntax. For example, when we want to perform addition in R we must use the + symbol; we can’t, for example, say 2 plus 2 and expect R to know what to do.\nSpeaking of addition, one of the easiest ways to start using R is to use it as a calculator! R, much like many other programming languages, obeys the standard order of operations when evaluating expressions:\n\nParentheses\nExponents\nMultiplication\nDivision\nAddition\nSubtraction\n\nHere is a list of mathematical operators and their corresponding R syntax:\n\n\n\nOperation\nR Operator\nExample\nResult\n\n\n\n\nAddition\n+\n2 + 2\n4\n\n\nSubtraction\n-\n2 - 2\n0\n\n\nMultiplication\n*\n2 * 2\n4\n\n\nDivision\n/\n2 / 2\n1\n\n\nExponentiation\n^\n2 ^ 2\n4\n\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\nEvaluate \\(\\displaystyle \\left(\\frac{2 + 3^{3/2}}{4}\\right)^{2}\\) using R."
  },
  {
    "objectID": "Files/R_Labs/L01_Basics/R_l01.html#variable-assignment",
    "href": "Files/R_Labs/L01_Basics/R_l01.html#variable-assignment",
    "title": "Programming in R",
    "section": "Variable Assignment",
    "text": "Variable Assignment\nLet’s talk a bit about variables. Just like in math, variables in a programming language function as a sort of placeholder for a particular piece of information (be it a function, value, etc.) The act of storing information in a variable is called assignment, and in R variable assignment is performed using the &lt;- symbol.\n\n&lt;variable name&gt; &lt;- &lt;what you want to associate with the variable&gt;\n\nFor example, after running\n\nx &lt;- 2\n\nthe quantity x will always be synonymous with the quantity 2, and running x + 2 will return a value of 4 (as 2 + 2 = 4).\nR affords a lot of flexibility when it comes to variable names- that is, we can pick almost anything we want to be a variable name! There are, however, some exceptions:\n\nVariable names cannot start with a number\nVariable names cannot include a space\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good programming practice to give your variables names that are descriptive, but not overly long.\n\n\nIf we want to view the value stored in a variable, we have two options: we could simply type the name of the variable, and run the cell:\n\nx\n\n[1] 2\n\n\nor we could pass the variable name into a call to the print() function (we’ll talk more about functions in a future workshop):\n\nprint(x)\n\n[1] 2\n\n\n\n\n\n\n\n\nCaution\n\n\n\nVariable names in R are case sensitive.\n\n\nFor example, if you run\n\nmy_variable &lt;- 25\n\nR will not know what to do with the variable My_variable!\n\nMy_variable\n\nError: object 'My_variable' not found\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe errors R outputs often contain useful information- be sure to read them!\n\n\nSometimes it will be necessary to update or re-assign a new value to an existing variable. For example, let’s examine the structure of the following code:\n\nx = 2\nx = x + 3\n\nWhat do you think running x will return? If you said 5, you’d be correct! The key point of this is:\n\n\n\n\n\n\nImportant\n\n\n\nWhen performing variable assignment, R reads from right to left.\n\n\nSo, in code example above, R first executed x + 3 (which is equivalent to 2 + 3; i.e. 5), and then re-assigned x the value 5.\n\n\n\n\n\n\nExercise 2\n\n\n\n\nDefine a variable called num_sections, and assign it the value of 3.\nDefine another variable called section_capacity, and assign it the value of 25.\nUpdate the value of num_sections to be one higher than it originally was.\nCompute the product of num_sections and section_capacity."
  },
  {
    "objectID": "Files/R_Labs/L01_Basics/R_l01.html#comments",
    "href": "Files/R_Labs/L01_Basics/R_l01.html#comments",
    "title": "Programming in R",
    "section": "Comments",
    "text": "Comments\nCode scripts can get long and complicated, pretty quickly. As such, it is often a good idea to add comments to your code. Comments are pretty much exactly what they sound like- they are short words or phrases that do not get executed, but can help the reader understand the code better.\nIn R, comments are created using the hashtag (#). For example:\n\nx &lt;- 1      # define x\nx &lt;- x + 1  # increment x by 1\n\nAgain, the phrases define x and increment x by 1 are never executed by R; they exist solely to help the reader understand what each line of code is doing."
  },
  {
    "objectID": "Files/R_Labs/L01_Basics/R_l01.html#functions",
    "href": "Files/R_Labs/L01_Basics/R_l01.html#functions",
    "title": "Programming in R",
    "section": "Functions",
    "text": "Functions\nFunctions in R behave much like functions in mathematics.\nGiven a function f() that has been defined in R (we will talk about how to define functions in a future workshop), we call the function on inputs (aka arguments) arg1, arg2, … using the syntax\n\nf(arg1, arg2, ...)\n\nFor example, as we saw previously, the print() function can be called on an argument to simply print the output:\n\nprint(\"hello\")\n\n[1] \"hello\""
  },
  {
    "objectID": "Files/R_Labs/L01_Basics/R_l01.html#help-files",
    "href": "Files/R_Labs/L01_Basics/R_l01.html#help-files",
    "title": "Programming in R",
    "section": "Help Files",
    "text": "Help Files\nR is an open-source software, meaning there are continually contributions being made. This means that there is no way to know everything about R! Rather, we must rely on the extensive documentation that is provided with most R packages and functions.\nTo access the help file on a particular function, you can use the syntax ?&lt;function name&gt;. For example, ?plot pulls up the help file for the plot() function in R.\n\n\n\n\n\n\nExercise 3\n\n\n\nLook up the help file for the sin() function.\n\n\nSometimes, we may want to look up the help file for function whose name we don’t know exactly. To look up help files containing a word, we can use ??&lt;name&gt;.\n\n\n\n\n\n\nExercise 4\n\n\n\nRun ??exponent, and find out how to perform exponentiation in R. Use this to compute \\(e^{4.2}\\)."
  },
  {
    "objectID": "Files/R_Labs/L01_Basics/R_l01.html#packages",
    "href": "Files/R_Labs/L01_Basics/R_l01.html#packages",
    "title": "Programming in R",
    "section": "Packages",
    "text": "Packages\nIn R, packages can be thought of as bundles containing several functions and/or structures. By default, R loads (i.e. includes) what is known as the base R package, which comes equipped with several useful functions.\nWe can access help files for particular packages using the same syntax as we used to look up the help files for functions: ?&lt;package name&gt;.\n\n\n\n\n\n\nExercise 5\n\n\n\nLook up the help file for the base package. Then, run code to display a list of all functions included in the base package.\n\n\n\n\n\n\n\n\nTip\n\n\n\nHelp files for packages are often referred to as vignettes by programmers who routinely use R.\n\n\nWith a few exceptions, we always need to install a package before using it for the first time. The syntax we use to install a package with the name package_name is\n\ninstall.packages(package_name)\n\n\n\n\n\n\n\nCaution\n\n\n\nYou only need to install a package once.\n\n\nAfter installing a package, we need to load or import it. The syntax we use to load a package with the name package_name is\n\nlibrary(package_name)"
  },
  {
    "objectID": "Files/R_Labs/L02_DT_DS/R_l02.html",
    "href": "Files/R_Labs/L02_DT_DS/R_l02.html",
    "title": "Programming in R",
    "section": "",
    "text": "Please make sure you have downloaded both R and RStudio.\nPlease make sure you are comfortable with the material in Part 1 (Fundamentals)."
  },
  {
    "objectID": "Files/R_Labs/L02_DT_DS/R_l02.html#prerequisites",
    "href": "Files/R_Labs/L02_DT_DS/R_l02.html#prerequisites",
    "title": "Programming in R",
    "section": "",
    "text": "Please make sure you have downloaded both R and RStudio.\nPlease make sure you are comfortable with the material in Part 1 (Fundamentals)."
  },
  {
    "objectID": "Files/R_Labs/L02_DT_DS/R_l02.html#data-types",
    "href": "Files/R_Labs/L02_DT_DS/R_l02.html#data-types",
    "title": "Programming in R",
    "section": "Data Types",
    "text": "Data Types\nMuch in the way different variables (in a statistical sense) can have different types (numerical vs. categorical), so to can quantities in R. For example, R treats the quantity 2 differently from the quantity \"hello\". We use the term data type to, loosely speaking, refer to the actual type of a particular quantity (e.g. numerical, character, etc.) The main data types in R are:\n\ndouble: refers to numerical (real-valued) quantities\ninteger: refers to integers\ncharacter: refers to character- or text-type data (and will always be enclosed in either single quotation marks or double quotation marks)\n\n\n\n\n\n\n\nImportant\n\n\n\nIn R, numbers are by default encoded with type double.\n\n\nTo check the type of a particular quantity, we can use the typeof() function. For example:\n\ntypeof(1.1)\n\n[1] \"double\"\n\ntypeof(1)\n\n[1] \"double\"\n\n\nAgain, note that the type of 1 is actually double! If we really wanted to reference the integer 1, we can use an L:\n\ntypeof(1L)\n\n[1] \"integer\""
  },
  {
    "objectID": "Files/R_Labs/L02_DT_DS/R_l02.html#data-structures",
    "href": "Files/R_Labs/L02_DT_DS/R_l02.html#data-structures",
    "title": "Programming in R",
    "section": "Data Structures",
    "text": "Data Structures\nWe can combine quantities in R as well, to produce what are known as data structures. Some of more common data structures in R are:\n\nVectors\nData Frames\nMatrices\nArrays\nLists\n\nEach data structure has a situation in which it is ideal; for now, let’s discuss vectors and data frames.\n\nVectors\nVectors are the most fundamental data structure in R. A vector is, effectively, a one-dimensional list of values. In R, we use the following syntax to create a vector:\n\nc(&lt;element 1&gt;, &lt;element 2&gt;, ...)\n\nFor example,\n\nc(1L, 3.5, \"hello\")\n\n[1] \"1\"     \"3.5\"   \"hello\"\n\n\n(By the Way: note how, in the output, R has converted all elements to be of type character! We’ll revisit this later.)\n\n\nData Frames\nAnother very popular way of storing data in R is using what is known as a data frame. A data frame can be thought of as a collection of vectors, arranged in tabular format; indeed, when storing data in a data frame, we are necessarily implementing the data matrix representation of data (see Workshop 02 for a refresher on this).\nData frames can be created using the data.frame() function in R:\n\ndata.frame(\n  colname1 = c(val1, val2, ...),\n  colname2 = c(val1, val2, ...),\n  ...\n)\n\nFor example:\n\ndata.frame(\n  col1 = c(2, 4, 6),\n  col2 = c(1, 3, 5)\n)\n\n  col1 col2\n1    2    1\n2    4    3\n3    6    5\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen displaying data frames, R always puts an initial column corresponding to row indices.\n\n\nNote that data frames are created column-wise. Additionally, the columns in a data frame must all be of the same length;\n\ndata.frame(\n  col1 = c(2, 4, 6),\n  col2 = c(1, 3, 5, 7)\n)\n\nError in data.frame(col1 = c(2, 4, 6), col2 = c(1, 3, 5, 7)): arguments imply differing number of rows: 3, 4\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe columns in a data frame can be comprised of different data types.\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\nMake a data frame in R based on the following table:\n\n\n\nname\nspecies\nage\n\n\n\n\nRover\nDog\n4\n\n\nSamson\nParrot\n1\n\n\nOlivia\nCat\n3\n\n\nHoppers\nRabbit\n2\n\n\n\nAssign your data frame to a variable called animals. That is, after completing this exercise, you should be able to run animals and obtain\n\nanimals\n\n     name species age\n1   Rover     Dog   4\n2  Samson  Parrot   1\n3  Olivia     Cat   3\n4 Hoppers  Rabbit   2"
  },
  {
    "objectID": "Files/R_Labs/L02_DT_DS/R_l02.html#slicingindexing",
    "href": "Files/R_Labs/L02_DT_DS/R_l02.html#slicingindexing",
    "title": "Programming in R",
    "section": "Slicing/Indexing",
    "text": "Slicing/Indexing\nWe may wish to extract or access only certain portions of a vector and/or data frame. This can be accomplished using slicing (aka indexing).\nGiven a vector x, the ith element of x can be extracted using the syntax x[i]. For example:\n\nx &lt;- c(1, 3, 5, 7)\nx[2]\n\n[1] 3\n\n\nGiven a data frame d, the (i, j)th element of d can be extracted using the syntax d[i, j]. For example:\n\nd &lt;- data.frame(\n  col1 = c(1, 3, 5, 7),\n  col2 = c(2, 4, 6, 8)\n)\n\nd[4, 2]\n\n[1] 8\n\n\nWe can also access columns in a data frame by using the $ operator. That is: given a data frame df with a column named col1, the synax df$col1 returns the col1 column of df. For example, given the d data frame defined above, we can access col2 by running\n\nd$col2\n\n[1] 2 4 6 8\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen returning columns of a data frame using slicing, R always returns a vector.\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nReturning to the animals data frame created above: return Olivia’s age in two different ways:\n\nby indexing\nby first extracting the age column, and then extracting the appropriate element."
  },
  {
    "objectID": "Files/R_Labs/L02_DT_DS/R_l02.html#importing-data",
    "href": "Files/R_Labs/L02_DT_DS/R_l02.html#importing-data",
    "title": "Programming in R",
    "section": "Importing Data",
    "text": "Importing Data\nIn many situations, we will want to import data that has been written or stored in a file or website. The basic function for reading data into R is read.table().\n\n\n\n\n\n\nExercise 3\n\n\n\nLook up the help file for the read.table() function. Additionally, look up the help file for the read.csv() function, and note how it differs from the read.table() function.\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\nImport the data located at https://pstat5a.github.io/Files/Datasets/movies_2000s.csv, and assign it to a variable called movies_2000. (Though you can technically download the data, try to import the data without downloading it onto your local machine). We will explore this data during the in-person portion of the Workshop in General Meeting 3."
  },
  {
    "objectID": "Pages/projects2223.html",
    "href": "Pages/projects2223.html",
    "title": "2022-2023 Projects",
    "section": "",
    "text": "Analyzing the Relationship Between Family Themes and Movie Ratings\n\n\n\n\n\n\nAnalyzing the Relationship Between Family Themes and Movie Ratings\n\nDo Daddy Issues Sell? This project sought to explore the relationship between movie ratings in genres pertaining to family themes and those not pertaining to family themes.\nGroup Members: Anna Xue, Kris Hao, Mollie Jiang, and Nixon Carino\n\n \n\n\n\n\n\n\n\n\n\n\n\nBest Bang for your Buck\n\n\n\n\n\n\nBest Bang for your Buck\n\nThis project sought to determine which fast food chains provide the “best bang for your buck”. More concretely, the nutritional content of meals from various fast food chains was analyzed, and restaurants were awarded one of “best health”, “best deals” and “best overall”.\nGroup Members: Brendan McGuinness, Alaina Liu\n\n \n\n\n\n\n\n\n\n\n\n\n\nReal or Fake Faces\n\n\n\n\n\n\nReal or Fake Faces\n\nWith the rise of AI-generated media, the ability to recognize real from fake is imperative. The goal of this project was to train a series of simple models to recognize photos of real faces from photos of artificially-generated faces.\nGroup Members: Yishan, Alice, Johanna P.\n\n \n\n\n\n\n\n\n\n\n\n\n\nTV Show and Movie Popularity Analysis\n\n\n\n\n\n\nTV Show and Movie Popularity Analysis\n\nDescription goes here.\nGroup Members: Christian Garduno, Roshan Mehta, Colton Rowe\n\n \n\n\n\n\n\n\n\n X",
    "crumbs": [
      "Projects",
      "2022-2023 Projects"
    ]
  },
  {
    "objectID": "Pages/officers.html",
    "href": "Pages/officers.html",
    "title": "Meet the Officers",
    "section": "",
    "text": "Cyrus Navasca\n  President\n  3rd Year - Statistics & Data Science + Math\n  Fun fact: Lost sense of smell after Covid\n  \n\n  \n  \n  \n  Owen Feng\n  Vice President\n  3rd Year - Statistics & Data Science\n  Fun fact: Has never had a good haircut\n  \n\n  \n  \n  \n  Pramukh Shankar\n  Treasurer\n  4th Year - Statistics & Data Science + Economics\n  Fun fact: Favorite Anime is Dr. Stone\n  \n\n  \n  \n  \n  Alice Xu\n  Outreach Chair\n  3rd Year - Statistics & Data Science + Economics\n  Fun fact: Never had Russian food before\n  \n\n  \n  \n  \n  Mollie Jiang\n  Secretary\n  4th Year - Statistics & Data Science\n  Fun fact: Currently studying abroad\n  \n\n  \n  \n  \n  Bowie Chuang\n  Website Chair\n  3rd Year - Statistics & Data Science + Economics\n  Fun fact: Really scared of spiders\n  \n\n  \n  \n  \n  Alex Bloyer\n  Website Officer\n  3rd Year - Statistics & Data Science\n  Fun fact: Plays kendama\n  \n\n  \n  \n  \n  Joyce De Quiros\n  Graphic Design\n  1st Year - Statistics & Data Science\n  Fun fact: Loves to make flan\n  \n\n  \n  \n  \n  Evan Jamison\n  Outreach Officer\n  2rd Year - Statistics & Data Science\n  Fun fact: Likes to surf & lift in the rec cen\n  \n\n  \n  \n  \n  Heidi Weng\n  Outreach Officer\n  2nd Year - Statistics & Data Science\n  Fun fact: Can speak 4 languages\n  \n\n  \n  \n  \n  Prisha Bobde\n  Events Chair\n  2nd Year - Statistics & Data Science\n  Fun fact: Trying new soup/curry recipes\n  \n\n  \n  \n  \n  Raymond Vargas\n  Events Officer\n  2nd Year - History & Statistics\n  Fun fact: Crocheting a sweater since 2021\n  \n\n  \n  \n  \n  Sabrina Ngyuen\n  Intern\n  2nd Year - Statistics & Data Science\n  Fun fact: Was bitten in the face by a dog at age 4 and paid off 3/4 of a school year with government money\n  \n\n  \n  \n  \n  Shashin Gupta\n  Intern\n  2nd Year - Statistics & Data Science\n  Fun fact: Name means Photo in Japanese\n  \n\n  \n  \n  \n  Christopher Santiago\n  Intern\n  2nd Year - Statistics & Data Science\n  Fun fact: Has a black belt in taekwondo\n  \n\n  \n  \n  \n  Ethan Marzban\n  Graduate Advisor\n  PhD - Statistics & Applied Probability\n  Fun fact: Loves to talk about cats\n  \n\n  \n  \n  \n  Dr. Uma Ravat\n  Faculty Advisor\n  Assistant Teaching Professor for the Department of Statistics & Applied Probability",
    "crumbs": [
      "Meet the Officers"
    ]
  },
  {
    "objectID": "Pages/projects.html",
    "href": "Pages/projects.html",
    "title": "Past Projects",
    "section": "",
    "text": "Past projects from our club members!"
  },
  {
    "objectID": "Pages/past.html",
    "href": "Pages/past.html",
    "title": "2022-2023 Meetings",
    "section": "",
    "text": "GM 3 - Manage Files and Repositories (3/3/23)\n\n\n\nSlides: [.pdf] (Click to download)  \n\n\n\n\n\n\nGM 2 - Create Project Proposal (2/3/23)\n\n\n\nSlides: [.pdf] (Click to download)   \n\n\n\n\n\n\nGM 1 - Introductory Workshop (1/27/23)\n\n\n\nSlides: [.pdf] (Click to download)",
    "crumbs": [
      "Meetings",
      "2022-2023 Meetings"
    ]
  },
  {
    "objectID": "Pages/past.html#general-meetings",
    "href": "Pages/past.html#general-meetings",
    "title": "2022-2023 Meetings",
    "section": "",
    "text": "GM 3 - Manage Files and Repositories (3/3/23)\n\n\n\nSlides: [.pdf] (Click to download)  \n\n\n\n\n\n\nGM 2 - Create Project Proposal (2/3/23)\n\n\n\nSlides: [.pdf] (Click to download)   \n\n\n\n\n\n\nGM 1 - Introductory Workshop (1/27/23)\n\n\n\nSlides: [.pdf] (Click to download)",
    "crumbs": [
      "Meetings",
      "2022-2023 Meetings"
    ]
  },
  {
    "objectID": "Pages/past.html#topic-specific-meetings",
    "href": "Pages/past.html#topic-specific-meetings",
    "title": "2022-2023 Meetings",
    "section": "Topic-Specific Meetings",
    "text": "Topic-Specific Meetings\n\n\n\n\nShiny Apps, and Slides (4/28/23)\n\n\n\nShiny Apps:\n\npalmerpenguins scatterplot, with option to color by species: [.R] (Click to download) \n\nSlides:\n\nBasic RSweave (.Rnw) file: [.Rnw] (Click to download)",
    "crumbs": [
      "Meetings",
      "2022-2023 Meetings"
    ]
  }
]